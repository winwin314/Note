# 李沐深度学习笔记

*前言：复习到第三节时才想起来用这个软件，所以笔记从第三节开始*

---

## 3. 线性神经网络

### 3.1 线性回归

> 向量x对应于单个数据样本的特征。 用符号表示的矩阵X可以很方便地引用我们整个数据集的个样本。 其中，X的每一行是一个样本，每一列是一种特征。

*  ==解析解==：可以直接解出损失函数的最小值（求导，导数为0点），从而对应于一个参数w。 这个参数就可以使得损失函数最小，总的来说，就是一步到位，直接找到最优解。
*  ==超参数==：超参数是影响模型性能的重要因素，选择合适的超参数可以帮助模型更好地拟合数据并避免过拟合，
    调节超参数是一项反复试验的工作，通常需要通过不同的技术和方法来进行优化。

* ==学习率==：它控制着每次参数更新的步长,学习率决定了每次更新时沿着梯度方向迈出多大的步伐。

* ==矢量化加速==：利用矢量运算（加减乘除）替代某些运算来实现运行加速的方法。

* ==极大似然估计==：还没学概率论前对这部分根本看不懂，这下看懂了。不过这里是“极大”，通过极大似然估计，

  可以找到参数局部最优值。这些参数计算得来的损失值就是更新后的、减小的损失值。

### 3.2 线性回归的从零实现

* `torch.normal(mean, std, size)` :生成数据服从正态分布，均值mean，方差std，大小为size的矩阵
  eg: torch. normal ( 0, 1, ( num_examples, len( w ) ) )  和  torch. normal( 0, 0.01, y. shape )

* `matmul()`可以实现更普适的张量乘法。 mm只适用于二维矩阵相乘。 

* 特征：X , 权重：***w*** , 标签：y

* 读取数据集（数据迭代器）：

  ```python
  def data_iter(batch_size, features, labels):
      num_examples = len(features)
      indices = list(range(num_examples))
      # 这些样本是随机读取的，没有特定的顺序,shuffle()用于打乱下标
      random.shuffle(indices)
      for i in range(0, num_examples, batch_size):
          batch_indices = torch.tensor(
              indices[i: min(i + batch_size, num_examples)])
          yield features[batch_indices], labels[batch_indices]
  ```

* 定义优化算法（优化器）：

  ```py
  def sgd(params, lr, batch_size):  #@save
      """小批量随机梯度下降"""
      with torch.no_grad():
          for param in params:
              param -= lr * param.grad / batch_size # 在函数外部要先反向传递
              param.grad.zero_()
  ```

* 训练：

  ```py
  lr = 0.03
  num_epochs = 3
  net = linreg
  loss = squared_loss
  
  for epoch in range(num_epochs):
      for X, y in data_iter(batch_size, features, labels):
          l = loss(net(X, w, b), y)  # X和y的小批量损失
          # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
          # 并以此计算关于[w,b]的梯度
          l.sum().backward() # 先反向传递才能得到grad，且这里是多对多所以要用sum()
          sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
      with torch.no_grad():
          train_l = loss(net(features, w, b), labels)
          print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
          #这里的误差会随着每一次更新越来越小
  ```

> 核心是小批量随机梯度下降：选取小批量的损失来计算梯度，以此更新整体的参数

### 3.3 线性回归的简洁实现

**本节核心** :实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。

* 生成数据集：

  ```py
  def load_array(data_arrays, batch_size, is_train=True):  #@save
      """构造一个PyTorch数据迭代器"""
      dataset = data.TensorDataset(*data_arrays) 
      return data.DataLoader(dataset, batch_size, shuffle=is_train)
  	#这个函数里的有关data的类和方法都用于处理data_arrays.
  
  batch_size = 10
  data_iter = load_array((features, labels), batch_size)
  ```

  >  解析：
  >
  > 1. `data.TensorDataset(*data_arrays)` 相当于 `data.TensorDataset(features, labels)` ,传入张量的元组，使用*来解包成两个张量。
  >
  > 2. `data.TensorDataset()` 里的两个张量第一维必须相等。

* 定义模型：

  ```py
  # nn是神经网络的缩写
  from torch import nn
  
  net = nn.Sequential(nn.Linear(2, 1))
  ```

  > 解析：在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。 但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉“标准的流水线”。

* 初始化模型参数：

  ```py
  net[0].weight.data.normal_(0, 0.01)
  net[0].bias.data.fill_(0)
  ```

* 定义损失函数(**均方误差损失**)：

  ```py
  loss = nn.MSELoss()
  ```

* 定义优化算法：

  ```py
  trainer = torch.optim.SGD(net.parameters(), lr=0.03)
  ```
  
  > 解析：优化算法只需传入params，见优化算法的具体实现

* 训练：

  ```py
  num_epochs = 3
  for epoch in range(num_epochs):
      for X, y in data_iter:
          l = loss(net(X) ,y)	# X,y是部分样本
          trainer.zero_grad()
          l.backward()
          trainer.step()
      l = loss(net(features), labels) #用部分样本优化的模型来计算关于全部样本的损失
      print(f'epoch {epoch + 1}, loss {l:f}')
  ```

### 3.4 softmax回归

* softmax函数：能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。
  $$
  softmax(o_j) = \frac{exp(o_j)}{\sum_{k}^{} exp(o_k)}
  $$

* 小批量样本的矢量化：
  $$
  O = XW + b,\\
  \hat{Y} = softmax(O)
  $$

* 交叉熵损失：负对数似然函数等价于损失函数(**令导数为0，求最小值**)。并且他的导数恰好就是softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值 $y$ 和估计值 $\hat{y}$ 之间的差异。

$$
l(y,\hat{y}) = -{\sum_{j=1}^{q}y_jlog\hat{y_j}}
$$

* 信息论：

  当事件的发生**概率**较低时，它带来的**惊异**和**信息量**都较大，因为它是不常见的、难以预测的事件。

  当事件的发生**概率**较高时，它带来的**惊异**和**信息量**较小，因为它是非常常见和容易预测的。

### 3.5 图像分类数据集

>  回顾之前线性回归的生成数据集操作

* `dataset`：数据集

* ` data.DataLoader()` ：有两个常见参数`shuffle`和`batch_size`.

* `data.TensorDataset()`：PyTorch 中的一个内置数据集类，用于将多个 `torch.Tensor` 对象组合在一起，形成一个数据集。

一个典型的数据集通常包含以下几个部分：特征（features）、标签（labels）、样本（samples）。其中每个样本由一组特征和对应的标签组成。

```py
trans = transforms.ToTensor() #trans转换器会将数据集中的每个图像转换成 Tensor 格式
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```

### 3.6 softmax回归的从零开始实现

```py
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
```

```py
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

```py
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])
```

> 对交叉熵的解析：
>
> 1. y_hat是包含若干个样本在各类别的预测概率，y_hat[range(len(y_hat)), y]可以挑选出各样本对正确类别的预测概率。y是各样本的正确预测标签（或者下标）。
> 2. 由于y是一个长度为q的独热编码向量， 所以除了一个项以外的所有项j都消失了。 由于所有$\hat{y}_j$都是预测的概率，所以它们的对数永远不会大于0。

* 分类精度：当预测与标签分类`y`一致时，即是正确的。 分类精度即正确预测数量与总预测数量之比。

```py
def accuracy(y_hat, y):  #@save
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

> `y_hat.argmax(axis=1)` 的作用是返回 `y_hat` 中每行最大值的索引。

* 重新回忆训练过程：

```py
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（定义见第3章）"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```

### 3.7 softmax回归的简洁实现

```py
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

#### 3.7.1 初始化模型参数

```py
# PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

> 解析：apply对每一层应用传入函数或方法。故传入函数的参数是“层”。

#### 3.7.2 重新审视Softmax的实现

```py
loss = nn.CrossEntropyLoss(reduction='none')
```

> 解析：在 PyTorch 中，`nn.CrossEntropyLoss` 具有一个名为 `reduction` 的参数，用于指定如何对每个样本的损失进行处理。它有三种常见的选项：
>
> - **`'none'`**: 不对损失进行任何聚合，每个样本的损失都会被单独返回。返回的是一个张量，其中每个元素是每个样本的损失。
> - **`'mean'`**: 返回所有样本损失的平均值，这是默认选项。
> - **`'sum'`**: 返回所有样本损失的总和。
>
> `reduction='none'` 表示每个样本的交叉熵损失都会被单独计算出来，返回一个包含每个样本损失的张量。

#### 3.7.3. 优化算法

```py
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```

#### 3.7.4. 训练

```py
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

> train_ch3里的整个训练过程详见[3.6 softmax回归的从零开始实现](#3.6 softmax回归的从零开始实现)，不完全一样，但差不多。
>
> 训练过程的几大要素都在体现在参数里面了，可以看参数回忆这几个重要的“机器”。

---



## 4. 多层感知机

### 4.1 多层感知机

#### 4.1.1 隐藏层

* 在面对复杂数据时，尤其是图像数据，传统的线性模型往往无法有效建模数据中的非线性关系和特征间的复杂交互作用。这时候需要用的隐藏层

* 我们按如下方式计算单隐藏层多层感知机的输出: O

$$
H = XW^{(1)} + b^{(1)} \\
O = HW^{(2)} + b^{(2)}
$$

> 注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数，即添加了隐藏层后上述模型仍然是仿射函数，但是我们之前的线性模型已经能够表示任何仿射函数。

* 添加激活函数就是从线性到非线性的过程，一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型。

$$
H = \sigma(XW^{(1)} + b^{(1)})\\
O = HW^{(2)} + b^{(2)}
$$

#### 4.1.2 激活函数

* ReLU函数：
  * 仅保留正元素并丢弃所有负元素。
  * ReLU求导表现得特别好：要么让参数消失，要么让参数通过，减轻了困扰以往神经网络的梯度消失问题。（稍后将介绍）

  <img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_21_0.svg" alt="image1" style="display: inline-block; margin-right: 10px; width: 45%;"> <img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_36_0.svg" alt="image2" style="display: inline-block; width: 45%;">
  
* sigmoid函数
  * 当输入接近0时，sigmoid函数接近线性变换。
  * 当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。
  
* tanh函数
  * 当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数， 不同的是tanh函数关于坐标系原点中心对称，sigmoid函数取值范围在0到1。
  * 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。

### 4.2 多层感知机的从零开始实现

#### 4.2.1 初始化模型参数

~~~py
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
~~~

> 解析：在PyTorch中，`nn.Parameter` 是一个特殊的 Tensor 类
>
> 注意：PyTorch 的优化器（如 `torch.optim.SGD`、`torch.optim.Adam` 等）只会更新模型的 `Parameter` 类型的张量。

#### 4.2.2 激活函数

```py
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

> 解析：为了确保对模型的细节了如指掌， 我们将实现ReLU激活函数， 而不是直接调用内置的`relu`函数。
>
> 注意relu的实现原理。

#### 4.2.3 模型

```py
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
```

#### 4.2.4 损失

```py
loss = nn.CrossEntropyLoss(reduction='none')
```

> 我们鼓励感兴趣的读者查看损失函数的源代码，以加深对实现细节的了解。

#### 4.2.5 训练

```py
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```

> 多层感知机的训练过程与softmax回归的训练过程完全相同。 可以直接调用`d2l`包的`train_ch3`函数.
>
> 加深对训练过程的了解程度。

### 4.3 多层感知机的简洁实现

#### 4.3.1 模型

```py
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

#### 4.3.2 训练

```py
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

### 4.4 模型选择、欠拟合和过拟合

#### 4.4.1 训练误差和泛化误差

* 将模型在训练数据上拟合的比在潜在分布中更接近的现象称为***过拟合***（overfitting）， 用于对抗过拟合的技术称为***正则化***（regularization）。

* 当我们有简单的模型和大量的数据时，我们期望泛化误差与训练误差相近。 当我们有更复杂的模型和更少的样本时，我们预计训练误差会下降，但泛化误差会增大。 

本节为了给出一些直观的印象，我们将重点介绍几个倾向于影响模型泛化的因素。

1. 可调整参数的数量。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。
2. 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
3. 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。

#### 4.4.2 模型选择

* **验证集** ：用于调优模型超参数和评估模型性能的一部分数据集。它通常是在训练集和测试集之间的一个独立数据集。

  * 验证集的作用：

    - **模型调优**：在==模型训练==过程中，验证集用于检查模型的表现，帮助选择最佳的超参数（如学习率、正则化强度、网络层数等）。

    - **防止过拟合**：通过监控验证集上的表现，我们可以避免模型在训练集上过度拟合（即模型过于复杂，能够很好地记住训练数据，但在未知数据上表现不佳）。如果在训练集上表现很好，但在验证集上表现较差，可能就存在==过拟合==的风险。

    - **选择最佳模型**：通常在训练过程中会对不同的模型架构或不同的超参数进行实验，验证集用于评估各个模型或超参数设置的性能，从而帮助==选择最优的配置==。

* **K折交叉验证**：原始训练数据被分成K个不重叠的子集。 然后执行K次模型训练和验证，每次在K−1个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对K次实验的结果取平均来估计训练和验证误差。

> 先训练和验证，再测试

#### 4.4.3 欠拟合还是过拟合？

**一. 模型复杂度**：

![](https://zh-v2.d2l.ai/_images/capacity-vs-error.svg)

> 高阶多项式函数比低阶多项式函数复杂得多。 高阶多项式的参数较多，模型函数的选择范围较广。 因此在固定训练数据集的情况下， 高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。

* **欠拟合**：训练误差和验证误差都很严重， 但它们之间仅有一点差距。

* **过拟合**：训练误差明显低于验证误差。

**二. 数据集大小**：

1. 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。

2. 模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。如果没有足够的数据，简单的模型可能更有用。

#### 4.4.4 多项式回归

一. 生成数据集：

```py
max_degree = 20  # 多项式的最大阶数
n_train, n_test = 100, 100  # 训练和测试数据集大小
true_w = np.zeros(max_degree)  # 分配大量的空间
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])

features = np.random.normal(size=(n_train + n_test, 1))
np.random.shuffle(features)
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))
for i in range(max_degree):
    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n)=(n-1)!
# labels的维度:(n_train+n_test,)
labels = np.dot(poly_features, true_w)
labels += np.random.normal(scale=0.1, size=labels.shape)
```

> 解析：
>
> 1. `np.power()`用到了广播机制。
> 2. `true_w` 的形状是 `(max_degree,)`，在进行点积时，NumPy 会自动将它看作列向量来处理，这样就可以和 `(n_samples, max_degree)` 形状的矩阵 `poly_features` 进行矩阵乘法。
> 3. ndarray数组是numpy里的核心数据结构；tensor是pytorch里的核心数据结构。

```py
# NumPy ndarray转换为tensor
true_w, features, poly_features, labels = [torch.tensor(x, dtype=
    torch.float32) for x in [true_w, features, poly_features, labels]]
```

二. 对模型进行训练和测试

* **评估模型的损失**：

```py
def evaluate_loss(net, data_iter, loss):  #@save
    """评估给定数据集上模型的损失"""
    metric = d2l.Accumulator(2)  # 损失的总和,样本数量
    for X, y in data_iter:
        out = net(X)
        y = y.reshape(out.shape)
        l = loss(out, y)
        metric.add(l.sum(), l.numel())
    return metric[0] / metric[1]
```

> 解析：计算每个小批量的损失，最后求均值

* **定义训练函数**:

```py
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = nn.MSELoss(reduction='none')
    input_shape = train_features.shape[-1]
    # 不设置偏置，因为我们已经在多项式中实现了它
    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),
                                batch_size)
    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),
                               batch_size, is_train=False)
    trainer = torch.optim.SGD(net.parameters(), lr=0.01)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net[0].weight.data.numpy())
```

> *tips：在定义这些函数时，要往函数功能的普遍性去设计，例如这里的feature其实就是一个列向量，特征数量为1，但我们还是要设计成可以普适所有情况的函数。*
>
> 1. `input_shape = train_features.shape[-1]` :train_features相当于X矩阵，取shape[-1]即取特征的数量。
> 2. `train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1),batch_size)` 这里的labels仅考虑了输出为标量的情况。**在单一标签回归和单标签分类问题中，输出都是标量。**
> 3. 训练过程：当epoch==0或每20个epoch训练完后计算一次损失。

三. 各种拟合：

