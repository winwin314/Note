# 李沐深度学习笔记

*前言：复习到第三节时才想起来用这个软件，所以笔记从第三章开始*

---

## 3. 线性神经网络

### 3.1 线性回归

> 向量x对应于单个数据样本的特征。 用符号表示的矩阵X可以很方便地引用我们整个数据集的个样本。 其中，X的每一行是一个样本，每一列是一种特征。

*  ==解析解==：可以直接解出损失函数的最小值（求导，导数为0点），从而对应于一个参数w。 这个参数就可以使得损失函数最小，总的来说，就是一步到位，直接找到最优解。
*  ==超参数==：超参数是影响模型性能的重要因素，选择合适的超参数可以帮助模型更好地拟合数据并避免过拟合，
    调节超参数是一项反复试验的工作，通常需要通过不同的技术和方法来进行优化。

* ==学习率==：它控制着每次参数更新的步长,学习率决定了每次更新时沿着梯度方向迈出多大的步伐。

* ==矢量化加速==：利用矢量运算（加减乘除）替代某些运算来实现运行加速的方法。

* ==极大似然估计==：还没学概率论前对这部分根本看不懂，这下看懂了。不过这里是“极大”，通过极大似然估计，

  可以找到参数局部最优值。这些参数计算得来的损失值就是更新后的、减小的损失值。

### 3.2 线性回归的从零实现

* `torch.normal(mean, std, size)` :生成数据服从正态分布，均值mean，方差std，大小为size的矩阵
  eg: torch. normal ( 0, 1, ( num_examples, len( w ) ) )  和  torch. normal( 0, 0.01, y. shape )

* `matmul()`可以实现更普适的张量乘法。 mm只适用于二维矩阵相乘。 

* 特征：X , 权重：***w*** , 标签：y

* 读取数据集（数据迭代器）：

  ```python
  def data_iter(batch_size, features, labels):
      num_examples = len(features)
      indices = list(range(num_examples))
      # 这些样本是随机读取的，没有特定的顺序,shuffle()用于打乱下标
      random.shuffle(indices)
      for i in range(0, num_examples, batch_size):
          batch_indices = torch.tensor(
              indices[i: min(i + batch_size, num_examples)])
          yield features[batch_indices], labels[batch_indices]
  ```

* 定义优化算法（优化器）：

  ```py
  def sgd(params, lr, batch_size):  #@save
      """小批量随机梯度下降"""
      with torch.no_grad():
          for param in params:
              param -= lr * param.grad / batch_size # 在函数外部要先反向传递
              param.grad.zero_()
  ```

* 训练：

  ```py
  lr = 0.03
  num_epochs = 3
  net = linreg
  loss = squared_loss
  
  for epoch in range(num_epochs):
      for X, y in data_iter(batch_size, features, labels):
          l = loss(net(X, w, b), y)  # X和y的小批量损失
          # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
          # 并以此计算关于[w,b]的梯度
          l.sum().backward() # 先反向传递才能得到grad，且这里是多对多所以要用sum()
          sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
      with torch.no_grad():
          train_l = loss(net(features, w, b), labels)
          print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
          #这里的误差会随着每一次更新越来越小
  ```

> 核心是小批量随机梯度下降：选取小批量的损失来计算梯度，以此更新整体的参数

### 3.3 线性回归的简洁实现

**本节核心** :实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。

* 生成数据集：

  ```py
  def load_array(data_arrays, batch_size, is_train=True):  #@save
      """构造一个PyTorch数据迭代器"""
      dataset = data.TensorDataset(*data_arrays) 
      return data.DataLoader(dataset, batch_size, shuffle=is_train)
  	#这个函数里的有关data的类和方法都用于处理data_arrays.
  
  batch_size = 10
  data_iter = load_array((features, labels), batch_size)
  ```

  >  解析：
  >
  > 1. `data.TensorDataset(*data_arrays)` 相当于 `data.TensorDataset(features, labels)` ,传入张量的元组，使用*来解包成两个张量。
  >
  > 2. `data.TensorDataset()` 里的两个张量第一维必须相等。

* 定义模型：

  ```py
  # nn是神经网络的缩写
  from torch import nn
  
  net = nn.Sequential(nn.Linear(2, 1))
  ```

  > 解析：在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。 但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉“标准的流水线”。

* 初始化模型参数：

  ```py
  net[0].weight.data.normal_(0, 0.01)
  net[0].bias.data.fill_(0)
  ```

* 定义损失函数(**均方误差损失**)：

  ```py
  loss = nn.MSELoss()
  ```

* 定义优化算法：

  ```py
  trainer = torch.optim.SGD(net.parameters(), lr=0.03)
  ```
  
  > 解析：优化算法只需传入params，见优化算法的具体实现

* 训练：

  ```py
  num_epochs = 3
  for epoch in range(num_epochs):
      for X, y in data_iter:
          l = loss(net(X) ,y)	# X,y是部分样本
          trainer.zero_grad()
          l.backward()
          trainer.step()
      l = loss(net(features), labels) #用部分样本优化的模型来计算关于全部样本的损失
      print(f'epoch {epoch + 1}, loss {l:f}')
  ```

### 3.4 softmax回归

* softmax函数：能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。
  $$
  softmax(o_j) = \frac{exp(o_j)}{\sum_{k}^{} exp(o_k)}
  $$

* 小批量样本的矢量化：
  $$
  O = XW + b,\\
  \hat{Y} = softmax(O)
  $$

* 交叉熵损失：负对数似然函数等价于损失函数(**令导数为0，求最小值**)。并且他的导数恰好就是softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值 $y$ 和估计值 $\hat{y}$ 之间的差异。

$$
l(y,\hat{y}) = -{\sum_{j=1}^{q}y_jlog\hat{y_j}}
$$

* 信息论：

  当事件的发生**概率**较低时，它带来的**惊异**和**信息量**都较大，因为它是不常见的、难以预测的事件。

  当事件的发生**概率**较高时，它带来的**惊异**和**信息量**较小，因为它是非常常见和容易预测的。

### 3.5 图像分类数据集

>  回顾之前线性回归的生成数据集操作

* `dataset`：数据集

* ` data.DataLoader()` ：有两个常见参数`shuffle`和`batch_size`.

* `data.TensorDataset()`：PyTorch 中的一个内置数据集类，用于将多个 `torch.Tensor` 对象组合在一起，形成一个数据集。

一个典型的数据集通常包含以下几个部分：特征（features）、标签（labels）、样本（samples）。其中每个样本由一组特征和对应的标签组成。

```py
trans = transforms.ToTensor() #trans转换器会将数据集中的每个图像转换成 Tensor 格式
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```

### 3.6 softmax回归的从零开始实现

```py
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
```

```py
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
	# W.shape[0]相当于特征数
```

```py
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])
```

> 对交叉熵的解析：
>
> 1. y_hat是包含若干个样本在各类别的预测概率，y_hat[range(len(y_hat)), y]可以挑选出各样本对正确类别的预测概率。y是各样本的正确预测标签（或者下标）。
> 2. 由于y是一个长度为q的独热编码向量， 所以除了一个项以外的所有项j都消失了。 由于所有$\hat{y}_j$都是预测的概率，所以它们的对数永远不会大于0。

* 分类精度：当预测与标签分类`y`一致时，即是正确的。 分类精度即正确预测数量与总预测数量之比。

```py
def accuracy(y_hat, y):  #@save
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

> `y_hat.argmax(axis=1)` 的作用是返回 `y_hat` 中每行最大值的索引。

* 重新回忆训练过程：

```py
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（定义见第3章）"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```

### 3.7 softmax回归的简洁实现

```py
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

#### 3.7.1 初始化模型参数

```py
# PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

> 解析：apply对每一层应用传入函数或方法。故传入函数的参数是“层”。

#### 3.7.2 重新审视Softmax的实现

```py
loss = nn.CrossEntropyLoss(reduction='none')
```

> 解析：在 PyTorch 中，`nn.CrossEntropyLoss` 具有一个名为 `reduction` 的参数，用于指定如何对每个样本的损失进行处理。它有三种常见的选项：
>
> - **`'none'`**: 不对损失进行任何聚合，每个样本的损失都会被单独返回。返回的是一个张量，其中每个元素是每个样本的损失。
> - **`'mean'`**: 返回所有样本损失的平均值，这是默认选项。
> - **`'sum'`**: 返回所有样本损失的总和。
>
> `reduction='none'` 表示每个样本的交叉熵损失都会被单独计算出来，返回一个包含每个样本损失的张量。

#### 3.7.3. 优化算法

```py
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```

#### 3.7.4. 训练

```py
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

> train_ch3里的整个训练过程详见[3.6 softmax回归的从零开始实现](#3.6 softmax回归的从零开始实现)，不完全一样，但差不多。
>
> 训练过程的几大要素都在体现在参数里面了，可以看参数回忆这几个重要的“机器”。

---



## 4. 多层感知机

### 4.1 多层感知机

#### 4.1.1 隐藏层

* 在面对复杂数据时，尤其是图像数据，传统的线性模型往往无法有效建模数据中的非线性关系和特征间的复杂交互作用。这时候需要用的隐藏层

* 我们按如下方式计算单隐藏层多层感知机的输出: O

$$
H = XW^{(1)} + b^{(1)} \\
O = HW^{(2)} + b^{(2)}
$$

> 注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。 可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！ 原因很简单：上面的隐藏单元由输入的仿射函数给出， 而输出（softmax操作前）只是隐藏单元的仿射函数。 仿射函数的仿射函数本身就是仿射函数，即添加了隐藏层后上述模型仍然是仿射函数，但是我们之前的线性模型已经能够表示任何仿射函数。

* 添加激活函数就是从线性到非线性的过程，一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型。

$$
H = \sigma(XW^{(1)} + b^{(1)})\\
O = HW^{(2)} + b^{(2)}
$$

#### 4.1.2 激活函数

* ReLU函数：
  * 仅保留正元素并丢弃所有负元素。
  * ReLU求导表现得特别好：要么让参数消失，要么让参数通过，减轻了困扰以往神经网络的梯度消失问题。（稍后将介绍）

  <img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_21_0.svg" alt="image1" style="display: inline-block; margin-right: 10px; width: 45%;"> <img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_36_0.svg" alt="image2" style="display: inline-block; width: 45%;">
  
* sigmoid函数
  * 当输入接近0时，sigmoid函数接近线性变换。
  * 当输入为0时，sigmoid函数的导数达到最大值0.25； 而输入在任一方向上越远离0点时，导数越接近0。

<img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_51_0.svg" alt="image1" style="display: inline-block; margin-right: 10px; width: 45%;"> <img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_66_0.svg" alt="image2" style="display: inline-block; width: 45%;">

* tanh函数
  * 当输入在0附近时，tanh函数接近线性变换。 函数的形状类似于sigmoid函数， 不同的是tanh函数关于坐标系原点中心对称，sigmoid函数取值范围在0到1。
  * 当输入接近0时，tanh函数的导数接近最大值1。 与我们在sigmoid函数图像中看到的类似， 输入在任一方向上越远离0点，导数越接近0。

<img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_81_0.svg" alt="image1" style="display: inline-block; margin-right: 10px; width: 45%;"> <img src="https://zh-v2.d2l.ai/_images/output_mlp_76f463_96_0.svg" alt="image2" style="display: inline-block; width: 45%;">

### 4.2 多层感知机的从零开始实现

#### 4.2.1 初始化模型参数

~~~py
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
~~~

> 解析：在PyTorch中，`nn.Parameter` 是一个特殊的 Tensor 类
>
> 注意：PyTorch 的优化器（如 `torch.optim.SGD`、`torch.optim.Adam` 等）只会更新模型的 `Parameter` 类型的张量。

#### 4.2.2 激活函数

```py
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

> 解析：为了确保对模型的细节了如指掌， 我们将实现ReLU激活函数， 而不是直接调用内置的`relu`函数。
>
> 注意relu的实现原理。

#### 4.2.3 模型

```py
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # 这里“@”代表矩阵乘法
    return (H@W2 + b2)
```

#### 4.2.4 损失

```py
loss = nn.CrossEntropyLoss(reduction='none')
```

> 我们鼓励感兴趣的读者查看损失函数的源代码，以加深对实现细节的了解。

#### 4.2.5 训练

```py
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)
```

> 多层感知机的训练过程与softmax回归的训练过程完全相同。 可以直接调用`d2l`包的`train_ch3`函数.
>
> 加深对训练过程的了解程度。

### 4.3 多层感知机的简洁实现

#### 4.3.1 模型

```py
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

#### 4.3.2 训练

```py
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

### 4.4 模型选择、欠拟合和过拟合

#### 4.4.1 训练误差和泛化误差

* 将模型在训练数据上拟合的比在潜在分布中更接近的现象称为***过拟合***（overfitting）， 用于对抗过拟合的技术称为***正则化***（regularization）。

* 当我们有简单的模型和大量的数据时，我们期望泛化误差与训练误差相近。 当我们有更复杂的模型和更少的样本时，我们预计训练误差会下降，但泛化误差会增大。 

本节为了给出一些直观的印象，我们将重点介绍几个倾向于影响模型泛化的因素。

1. 可调整参数的数量。当可调整参数的数量（有时称为*自由度*）很大时，模型往往更容易过拟合。
2. 参数采用的值。当权重的取值范围较大时，模型可能更容易过拟合。
3. 训练样本的数量。即使模型很简单，也很容易过拟合只包含一两个样本的数据集。而过拟合一个有数百万个样本的数据集则需要一个极其灵活的模型。

#### 4.4.2 模型选择

* **验证集** ：用于调优模型超参数和评估模型性能的一部分数据集。它通常是在训练集和测试集之间的一个独立数据集。
  * 验证集的作用：
  
    - **模型调优**：在==模型训练==过程中，验证集用于检查模型的表现，帮助选择最佳的超参数（如学习率、正则化强度、网络层数等）。
  
    - **防止过拟合**：通过监控验证集上的表现，我们可以避免模型在训练集上过度拟合（即模型过于复杂，能够很好地记住训练数据，但在未知数据上表现不佳）。如果在训练集上表现很好，但在验证集上表现较差，可能就存在==过拟合==的风险。
  
    - **选择最佳模型**：通常在训练过程中会对不同的模型架构或不同的超参数进行实验，验证集用于评估各个模型或超参数设置的性能，从而帮助==选择最优的配置==。
  
* **K折交叉验证**：原始训练数据被分成K个不重叠的子集。 然后执行K次模型训练和验证，每次在K−1个子集上进行训练， 并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。 最后，通过对K次实验的结果取平均来估计训练和验证误差。

> 先训练和验证，再测试

#### 4.4.3 欠拟合还是过拟合？

**一. 模型复杂度**：

![](https://zh-v2.d2l.ai/_images/capacity-vs-error.svg)

> 高阶多项式函数比低阶多项式函数复杂得多。 高阶多项式的参数较多，模型函数的选择范围较广。 因此在固定训练数据集的情况下， 高阶多项式函数相对于低阶多项式的训练误差应该始终更低（最坏也是相等）。

* **欠拟合**：训练误差和验证误差都很严重， 但它们之间仅有一点差距。

* **过拟合**：训练误差明显低于验证误差。

**二. 数据集大小**：

1. 训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。 随着训练数据量的增加，泛化误差通常会减小。

2. 模型复杂性和数据集大小之间通常存在关系。 给出更多的数据，我们可能会尝试拟合一个更复杂的模型。如果没有足够的数据，简单的模型可能更有用。

#### 4.4.4 多项式回归

一. 生成数据集：

```py
max_degree = 20  # 多项式的最大阶数
n_train, n_test = 100, 100  # 训练和测试数据集大小
true_w = np.zeros(max_degree)  # 分配大量的空间
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6])

features = np.random.normal(size=(n_train + n_test, 1))
np.random.shuffle(features)
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1))
for i in range(max_degree):
    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n)=(n-1)!
# labels的维度:(n_train+n_test,)
labels = np.dot(poly_features, true_w)
labels += np.random.normal(scale=0.1, size=labels.shape)
```

> 解析：
>
> 1. `np.power()`用到了广播机制。
> 2. `true_w` 的形状是 `(max_degree,)`，在进行点积时，NumPy 会自动将它看作列向量来处理，这样就可以和 `(n_samples, max_degree)` 形状的矩阵 `poly_features` 进行矩阵乘法。
> 3. ndarray数组是numpy里的核心数据结构；tensor是pytorch里的核心数据结构。

```py
# NumPy ndarray转换为tensor
true_w, features, poly_features, labels = [torch.tensor(x, dtype=
    torch.float32) for x in [true_w, features, poly_features, labels]]
```

二. 对模型进行训练和测试

* **评估模型的损失**：

```py
def evaluate_loss(net, data_iter, loss):  #@save
    """评估给定数据集上模型的损失"""
    metric = d2l.Accumulator(2)  # 损失的总和,样本数量
    for X, y in data_iter:
        out = net(X)
        y = y.reshape(out.shape)
        l = loss(out, y)
        metric.add(l.sum(), l.numel())
    return metric[0] / metric[1]
```

> 解析：计算每个小批量的损失，最后求均值

* **定义训练函数**:

```py
def train(train_features, test_features, train_labels, test_labels,
          num_epochs=400):
    loss = nn.MSELoss(reduction='none')
    input_shape = train_features.shape[-1]
    # 不设置偏置，因为我们已经在多项式中实现了它
    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1)),
                                batch_size)
    test_iter = d2l.load_array((test_features, test_labels.reshape(-1,1)),
                               batch_size, is_train=False)
    trainer = torch.optim.SGD(net.parameters(), lr=0.01)
    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',
                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],
                            legend=['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss),
                                     evaluate_loss(net, test_iter, loss)))
    print('weight:', net[0].weight.data.numpy())
```

> *tips：在定义这些函数时，要往函数功能的普遍性去设计，例如这里的feature其实就是一个列向量，特征数量为1，但我们还是要设计成可以普适所有情况的函数。*
>
> 1. `input_shape = train_features.shape[-1]` :train_features相当于X矩阵，取shape[-1]即取特征的数量。
> 2. `train_iter = d2l.load_array((train_features, train_labels.reshape(-1,1),batch_size)` 这里的labels仅考虑了输出为标量的情况。**在单一标签回归和单标签分类问题中，输出都是标量。**
> 3. 训练过程：当epoch==0或每20个epoch训练完后计算一次损失。

三. 各种拟合：

1. 三项多项式函数拟合(正常)：

```py
# 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
      labels[:n_train], labels[n_train:])
```

2. 线性函数拟合(欠拟合)：

```py
# 从多项式特征中选择前2个维度，即1和x
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
      labels[:n_train], labels[n_train:])
```

3. 高阶多项式函数拟合(过拟合)：

```py
# 从多项式特征中选取所有维度
train(poly_features[:n_train, :], poly_features[n_train:, :],
      labels[:n_train], labels[n_train:], num_epochs=1500)
```

<img src="https://zh-v2.d2l.ai/_images/output_underfit-overfit_ec26bd_78_1.svg" alt="image1" style="display: inline-block; width: 30%; margin-right: 10px;"><img src="https://zh-v2.d2l.ai/_images/output_underfit-overfit_ec26bd_93_1.svg" alt="image2" style="display: inline-block; width: 30%; margin-right: 10px;"><img src="https://zh-v2.d2l.ai/_images/output_underfit-overfit_ec26bd_108_1.svg" alt="image3" style="display: inline-block; width: 30%;">

### 4.5 权重衰减

#### 4.5.1 L_p正则化

> *引入：本节将介绍一些正则化模型的技术用以缓解过拟合。当然我们可以通过收集更多的数据来缓解过拟合，（注意[影响模型拟合的两个基本性质](#4.4.3 欠拟合还是过拟合？)*）但这样成本较高，因此我们便可将重点放在正则化技术上。
> 只是简单地限制特征的数量可能仍然使模型在过简单和过复杂中徘徊，我们需要更细粒度的工具来调整函数的复杂度，$L_p$范数。我们通过添加惩罚项来降低权重向量的范数$L_p$，以此达到降低模型复杂度的目的。

* 在训练参数化机器学习模型时，***权重衰减***（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为L2*正则化*；对于λ=0，我们恢复了原来的损失函数，此时不会有正则化的效果。 对于λ>0，我们限制‖w‖的大小，λ越大，衰减得越厉害。

$$
L(w,b) + \frac{\lambda}{2}||w||^{2}
$$

* 使用L2范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，L1惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为***特征选择***（feature selection），这可能是其他场景下需要的。

* 与特征选择相比，权重衰减为我们提供了一种**连续的机制**来调整函数的复杂度。 较小的λ值对应较少约束的w， 而较大的λ值对w的约束更大。通常，网络输出层的偏置项不会被正则化。

#### 4.5.2 从零开始实现

首先，我们像以前一样生成一些数据，生成公式如下：

$$
y = 0.05 + \sum_{i=1}^{d}0.01x_i + \epsilon
$$

我们选择标签是关于输入的线性函数。 标签同时被"均值为0，标准差为0.01高斯噪声破坏"。 为了使过拟合的效果更加明显，我们可以将问题的维数增加到d=200， 并使用一个只包含20个样本的小训练集。

```py
n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5 #20个训练样本，有200个特征
true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05
train_data = d2l.synthetic_data(true_w, true_b, n_train)
train_iter = d2l.load_array(train_data, batch_size)
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train=False)
```

##### 4.5.2.1 初始化模型参数

```py
def init_params():
    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]
```

> 解析：之前初始化参数时用的是自定义init_weights()和net.apply() -->[多层感知机的简洁实现](#4.3.1 模型)

##### 4.5.2.2 定义$L_2$范数惩罚

```py
def l2_penalty(w):
    return torch.sum(D.pow(2)) / 2
```

##### 4.5.2.3 定义训练代码实现

```py
def train(lambd):
    w, b = init_params()
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    num_epochs, lr = 100, 0.003
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    #animator用于动画展示，可不看
    for epoch in range(num_epochs):
        for X, y in train_iter:
            # 增加了L2范数惩罚项，
            # 广播机制使l2_penalty(w)成为一个长度为batch_size的向量，这里的loss是各个样本的损失			  组成的向量"y^hat - y"
            l = loss(net(X), y) + lambd * l2_penalty(w)
            l.sum().backward()
            d2l.sgd([w, b], lr, batch_size)
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss),
                                     d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：', torch.norm(w).item())
```

##### 4.5.2.4 忽略正则化直接训练

* 这里训练误差有了减少，但测试误差没有减少， 这意味着出现了严重的过拟合。

```py
train(lambd=0)
```

![](https://zh-v2.d2l.ai/_images/output_weight-decay_ec9cc0_81_1.svg)

##### 4.5.2.5 使用权重衰减

* 在这里训练误差增大(没有为了拟合训练数据而造成过拟合)，但测试误差减小。 这正是我们期望从正则化中得到的效果。

```py
train(lambd=3)
```

![](https://zh-v2.d2l.ai/_images/output_weight-decay_ec9cc0_96_1.svg)

#### 4.5.3 简洁实现

```py
def train_concise(wd):
    net = nn.Sequential(nn.Linear(num_inputs, 1))
    for param in net.parameters():
        param.data.normal_()
    loss = nn.MSELoss(reduction='none')
    num_epochs, lr = 100, 0.003
    # 偏置参数没有衰减
    trainer = torch.optim.SGD([
        {"params":net[0].weight,'weight_decay': wd},
        {"params":net[0].bias}], lr=lr)
    animator = d2l.Animator(xlabel='epochs', ylabel='loss', yscale='log',
                            xlim=[5, num_epochs], legend=['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.mean().backward()
            trainer.step()
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1,
                         (d2l.evaluate_loss(net, train_iter, loss),
                          d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', net[0].weight.norm().item())
```

> 解析：注意优化器的定义。

### 4.6 暂退法（Dropout）

#### 4.6.1 扰动的稳健性

在上一节，我们知道了**参数的范数**代表了一种有用的模型简单性度量。

简单性的另一个角度是**平滑性**，即函数不应该对其输入的微小变化敏感。1995年，克里斯托弗·毕晓普证明了具有输入噪声的训练等价于Tikhonov正则化。这项工作用数学证实了“要求函数光滑”和“要求函数对输入的随机噪声具有适应性”之间的联系。

* 斯里瓦斯塔瓦等人提出：在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。这个想法被称为***暂退法***（dropout）。 这种方法之所以被称为*暂退法*，因为我们从表面上看是在训练过程中**丢弃**（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。

$$
x^{’} = \left\{
\begin{matrix}
0 & \text 概率为p \\
\frac{x}{1-p} & \text 其他
\end{matrix}
\right.
$$

这样可保证$E[x']=x$, 具体表现为一些特征被丢弃，剩下的特征变大。

![](https://zh-v2.d2l.ai/_images/dropout2.svg)

> 通常，我们在测试时不用暂退法

#### 4.6.2 从零开始实现

```py
import torch
from torch import nn
from d2l import torch as d2l


def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 在本情况中，所有元素都被丢弃
    if dropout == 1:
        return torch.zeros_like(X)
    # 在本情况中，所有元素都被保留
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float()
    return mask * X / (1.0 - dropout)
```

> 解析：torch.rand(X.shape) 会产生一个形状和X相同的、数值大小在0 ~ 1间随机取得的张量。这里用了一个巧妙的方法来模拟“概率为p时"的情况。在0 ~ 1 的数轴上，落在0 ~ dropout上的数值概率为dropout，落在dropout ~ 1上的概率为（1-dropout）。

通过下面几个例子来测试`dropout_layer`函数。 我们将输入`X`通过暂退法操作，暂退概率分别为0、0.5和1。

```py
X= torch.arange(16, dtype = torch.float32).reshape((2, 8))
print(X)
print(dropout_layer(X, 0.))
print(dropout_layer(X, 0.5))
print(dropout_layer(X, 1.))
```

```py
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
tensor([[ 0.,  2.,  0.,  6.,  0.,  0.,  0., 14.],
        [16., 18.,  0., 22.,  0., 26., 28., 30.]])
tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.]])
```

##### 4.6.2.1 定义模型参数

```py
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256
```

##### 4.6.2.2 定义模型

```py
dropout1, dropout2 = 0.2, 0.5

class Net(nn.Module):
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 is_training = True):
        super(Net, self).__init__()
        self.num_inputs = num_inputs
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1)
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # 只有在训练模型时才使用dropout
        if self.training == True:
            # 在第一个全连接层之后添加一个dropout层
            H1 = dropout_layer(H1, dropout1)
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # 在第二个全连接层之后添加一个dropout层
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2)
        return out


net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)
```

> 解析：
>
> 1. `nn.Module` 是 PyTorch 中所有神经网络模块的基类.
> 2. `super(Net, self).__init__()` 父类初始化.
> 3. `X.reshape((-1, self.num_inputs)` self.num_inputs表特征数量，-1表自适应样本数，通常是batch_size.

##### 4.6.2.3 训练和测试

```py
num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss(reduction='none')
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

#### 4.6.3 简洁实现

```py
net = nn.Sequential(nn.Flatten(),
        nn.Linear(784, 256),
        nn.ReLU(),
        # 在第一个全连接层之后添加一个dropout层
        nn.Dropout(dropout1),
        nn.Linear(256, 256),
        nn.ReLU(),
        # 在第二个全连接层之后添加一个dropout层
        nn.Dropout(dropout2),
        nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

训练和测试：

```py
trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

### 4.7 前向传播、反向传播和计算图

1. **前向传播**：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

2. **反向传播**：指的是计算神经网络参数梯度的方法。（梯度即导数）

3. **计算图**：

   ![](https://zh-v2.d2l.ai/_images/forward.svg)

> （1）只有叶子节点才可以计算梯度.
>
> （2）只有标量才可以反向传播backward.

计算梯度可以按照计算图的反方向确定“所需偏导”。注意，计算对某变量偏导可能有多条路径，需要对两条路径的偏导相加（这是数学知识）

### 4.8 数值稳定性和模型初始化

#### 4.8.1 梯度消失和梯度爆炸

***==重点==**：
$$
\frac{\partial l}{\partial W^t} = \frac{\partial l}{\partial h^d}\frac{\partial h^{d}}{\partial h^{d-1}}...\frac{\partial h^{t+1}}{\partial h^{t}}\frac{\partial h^t}{\partial W^t}
$$
（$h^d$是输出层）

> 解析：
>
> 1. $\frac{\partial l}{\partial W^t}$ 是权重更新的梯度，$\frac{\partial h^{i+1}}{\partial h^{i}}$  是激活函数的梯度（每一层之间的桥梁）。这就是“梯度累加”。
> 2. 当激活函数的梯度很大时，不断累加的梯度就会导致最后权重更新的梯度过大，造成梯度爆炸；
> 3. 当激活函数的梯度很小时，不断累加的梯度就会导致最后权重更新的梯度过小，造成梯度消失；

“权重初始化范围过大 + 激活函数不合适 = 梯度爆炸/消失”

* **打破对称性**：这一个知识点不知道为什么要加到《梯度消失和梯度爆炸》这个标题里面，感觉有点牵强。但对称性和梯度消失/爆炸有一个共同点：不合理的模型初始化导致的。 

  > 那么什么是对称性？简单来说，就是**网络中的各个神经元在训练过程中学习到了相同的特征**，这通常发生在网络的权重初始化阶段，尤其是当所有神经元的权重被初始化为相同的值时。由于每个神经元的输出都是相同的，它们的梯度也会相同，因此在反向传播中，所有神经元的权重更新量也会相同，最终导致每个神经元学习到的特征相同。

  这会导致网络的表达能力受限，性能下降。为了避免这个问题，通常需要对权重进行随机初始化（如 Xavier 初始化），以打破对称性并提高网络的性能。

#### 4.8.2 参数初始化

解决（或至少减轻）上述问题的一种方法是进行参数初始化， 优化期间的注意和适当的正则化也可以进一步提高稳定性。

##### 4.8.2.1默认初始化

在前面的部分中，例如在[3.3节](#3.3 线性回归的简洁实现)中， 我们使用正态分布来初始化权重值。如果我们不指定初始化方法， 框架将使用默认的随机初始化方法，对于中等难度的问题，这种方法通常很有效。

##### 4.8.2.2 Xavier初始化

目标：1. 所有层的**输入输出**都有相同的均值和方差。2.所有权重的**更新梯度**(损失函数关于权重的导数)都有相同的均值和方差。

下面是Xavier初始化的由来推导：

从无激活函数的全连接层输出入手：
$$
o_i = \sum_{j=1}^{n_{in}}w_{ij}x_j
$$

> n_in里的in就是单词in的意思，即输入特征数量，上述公式其实就是输入向量x乘以权重矩阵W得到输出向量o的过程。

（1）权重$w_{ij}$都是从同一分布独立抽取，假设该分布零均值，有方差$\sigma^2$。层$x_j$的输入也有零均值和方差$\gamma^2$,他们独立于$w_{ij}$且彼此独立。这种情况下，我们可以按如下方式计算$o_i$的平均值和方差：(详细推导略)
$$
E[o_i] = \sum_{j=1}^{n_{in}}E[w_{ij}x_j] = 0 \\
D[o_i] = E[o_i^2] - (E[o_i])^2 = （n_{in}\sigma^2）\gamma^2
$$
可以看到当前层输出的**均值E同上一层**，方差D是**上一层输入的方差$\gamma^2$**乘以$n_{in}\sigma^2$，想要保持输出方差不变，一种方法是设置$n_{in}\sigma^2 = 1$。

（2）现在考虑反向传播过程，我们知道，当前层的输出依赖的是上一层的输出，而上一层的输出又依赖于上上一层输出，如此累加。反向传播梯度也是如此，这里有一个重要的公式
$$
\frac{\partial l}{\partial h^{t-1}} = \frac{\partial l}{\partial h^{t}}W^t
$$
即，**靠近输入的更新梯度依赖于靠近输出的更新梯度，并且和输入输出的关系一样，都依靠权重矩阵来维系着**
$$
h^{t} = h^{t-1}·W^t
$$
因此均值和方差的推导同上，可知，要向保持更新梯度的方差不变，需要满足$n_{out}\sigma^2 = 1$。

这使得我们进退两难：我们不可能同时满足这两个条件。那么，为了尽可能满足这两个式子，我们需满足：
$$
\frac{1}{2}(n_{in} + n_{out})\sigma^2 = 1
$$
通常，Xavier初始化从均值为零，方差$\sigma^2 = \frac{2}{n_{in} + n_{out}}$的高斯分布中采样权重。也可以将其改为选择从均匀分布中抽取权重，方差同上，d。

> 这一节如何理解得透彻在于掌握前向传播和反向传播的直观过程：像进度条一样向前/后推进。

### 4.9 环境和分布偏移

**这一节没有视频，翻译得也不是很好，目测也不是很重要，故略。**

### 4.10 实战Kaggle比赛：预测房价

#### 4.10.1 下载和缓存数据集

```py
import hashlib
import os
import tarfile
import zipfile
import requests

#@save
DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'
```

> 解析：DATA_HUB用来装数据集，每个键对应一个值（二元组），二元组里包括url和sha1_hash

下面将数据集缓存在本地目录（默认情况下为`../data`）中， 并返回下载文件的名称。 如果缓存目录中已经存在此数据集文件，并且其sha-1与存储在`DATA_HUB`中的相匹配， 我们将使用缓存的文件，以避免重复的下载。

```py
def download(name, cache_dir=os.path.join('..', 'data')):  #@save
    """下载一个DATA_HUB中的文件，返回本地文件名"""
    assert name in DATA_HUB, f"{name} 不存在于 {DATA_HUB}"
    url, sha1_hash = DATA_HUB[name]
    os.makedirs(cache_dir, exist_ok=True)
    fname = os.path.join(cache_dir, url.split('/')[-1])
    if os.path.exists(fname):
        sha1 = hashlib.sha1()
        with open(fname, 'rb') as f:
            while True:
                data = f.read(1048576)
                if not data:
                    break
                sha1.update(data)
        if sha1.hexdigest() == sha1_hash:
            return fname  # 命中缓存
    print(f'正在从{url}下载{fname}...')
    r = requests.get(url, stream=True, verify=True)
    with open(fname, 'wb') as f:
        f.write(r.content)
    return fname
```

> 解析：
>
> 1. `url.split('/')[-1]`取url的最后一个文件名(?) 
> 2. `sha1 = hashlib.sha1()`创建一个哈希对象，作用相当于一个哈希函数，但要`.update()`上去，再 `.hexdigest()` 
> 3. `data = f.read(1048576)`：一个字节一个字节读取数据
> 4. `r = requests.get(url, stream=True, verify=True)` 创建一个reponse对象，用于响应url（可能是某种获取）。`r.content` 就是字面意思取得内容。

我们还需实现两个实用函数： 一个将下载并解压缩一个zip或tar文件， 另一个是将本书中使用的所有数据集从`DATA_HUB`下载到缓存目录中。

```py
def download_extract(name, folder=None):  #@save
    """下载并解压zip/tar文件"""
    fname = download(name)
    base_dir = os.path.dirname(fname)
    data_dir, ext = os.path.splitext(fname)
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')
    elif ext in ('.tar', '.gz'):
        fp = tarfile.open(fname, 'r')
    else:
        assert False, '只有zip/tar文件可以被解压缩'
    fp.extractall(base_dir)
    return os.path.join(base_dir, folder) if folder else data_dir

def download_all():  #@save
    """下载DATA_HUB中的所有文件"""
    for name in DATA_HUB:
        download(name)
```

> 解析：
>
> 1. `data_dir, ext = os.path.splitext(fname)` 把fname拆成前半部分和”.文件类型“
> 2. `fp = zipfile.ZipFile(fname, 'r')` 打开一个 ZIP 文件，并创建一个 `ZipFile` 对象，用于读取或操作 ZIP 文件的内容。
> 3. `fp = tarfile.open(fname, 'r')` 同上。
> 4. `fp.extractall(base_dir)` zip和tar都有的解压缩方法。

#### 4.10.2 Kaggle

**\*略\***

#### 4.10.3 访问和读取数据集

```py
# 如果没有安装pandas，请取消下一行的注释
# !pip install pandas

%matplotlib inline
import numpy as np
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l
```

下面这段有点看不懂

```py
DATA_HUB['kaggle_house_train'] = (  #@save
    DATA_URL + 'kaggle_house_pred_train.csv',
    '585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (  #@save
    DATA_URL + 'kaggle_house_pred_test.csv',
    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')
```

我们使用`pandas`分别加载包含训练数据和测试数据的两个CSV文件。

```py
train_data = pd.read_csv(download('kaggle_house_train'))
test_data = pd.read_csv(download('kaggle_house_test'))
```

> 解析：pandas的数据结构对应dataframe，类似于一个电子表格

训练数据集包括1460个样本，每个样本80个特征和1个标签， 而测试数据集包含1459个样本，每个样本80个特征。(这里没有提供标签)

```py
print(train_data.shape)
print(test_data.shape)
```

```py
(1460, 81)
(1459, 80)
```

让我们看看前四个和最后两个特征，以及相应标签（房价）。

```py
print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])
```

```py
   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice
0   1          60       RL         65.0       WD        Normal     208500
1   2          20       RL         80.0       WD        Normal     181500
2   3          60       RL         68.0       WD        Normal     223500
3   4          70       RL         60.0       WD       Abnorml     140000
```

在每个样本中，第一个特征是ID， 这有助于模型识别每个训练样本。 虽然这很方便，但它不携带任何用于预测的信息。 因此，在将数据提供给模型之前，我们将其从数据集中删除。

```py
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))
```

#### 4.10.4 数据预处理

1. 对数值型特征：将特征重新缩放到零均值和单位方差，将缺失值设置为0.

```py
# 若无法获得测试数据，则可根据训练数据计算均值和标准差
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_features] = all_features[numeric_features].apply(
    lambda x: (x - x.mean()) / (x.std()))
# 在标准化数据之后，所有均值消失，因此我们可以将缺失值设置为0
all_features[numeric_features] = all_features[numeric_features].fillna(0)
```

> 解析：numeric_features:表所有数值型特征的列下标，因为我们现在在操作数值型特征，所以都在围绕`all_features[numeric_features].`

2. 对非数值型特征：

​	可以看到特征数量变多了

```py
# “Dummy_na=True”将“na”（缺失值）视为有效的特征值，并为其创建指示符特征
all_features = pd.get_dummies(all_features, dummy_na=True)
all_features.shape
```

```py
(2919, 331)
```

3. 将数据转换成tensor形式便于训练

```py
n_train = train_data.shape[0]
train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)
test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)
train_labels = torch.tensor(
    train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)
```

> 解析：
>
> 1. 为什么要把训练数据和测试数据合起来再分开，这个操作是否多余？：对于上面前两个阶段，对于数据的处理不区分训练数据和测试数据，因为所有数据都需要经过这两步处理，所以合起来一起处理效率更高。
> 2. `all_features[:n_train].values` 将 `pd.DataFrame` 转换为一个二维 `np.ndarray` 数组，dtype为float64。所以上面这段代码实际上包含了两次转换。

#### 4.10.5 训练

首先，我们训练一个带有损失平方的线性模型。 显然线性模型很难让我们在竞赛中获胜，但线性模型提供了一种健全性检查， 以查看数据中是否存在有意义的信息。 如果我们在这里不能做得比随机猜测更好，那么我们很可能存在数据处理错误。 如果一切顺利，线性模型将作为*基线*（baseline）模型， 让我们直观地知道最好的模型有超出简单的模型多少。

```py
loss = nn.MSELoss()
in_features = train_features.shape[1]

def get_net():
    net = nn.Sequential(nn.Linear(in_features,1))
    return net
```

房价我们关心相对数量而不是绝对数量。例如，如果我们在俄亥俄州农村地区估计一栋房子的价格时， 假设我们的预测偏差了10万美元， 然而那里一栋典型的房子的价值是12.5万美元， 那么模型可能做得很糟糕。 另一方面，如果我们在加州豪宅区的预测出现同样的10万美元的偏差， （在那里，房价中位数超过400万美元） 这可能是一个不错的预测。

解决这个问题的一种方法是用价格预测的**对数**来衡量差异。

```py
def log_rmse(net, features, labels):
    # 为了在取对数时进一步稳定该值，将小于1的值设置为1
    clipped_preds = torch.clamp(net(features), 1, float('inf'))
    rmse = torch.sqrt(loss(torch.log(clipped_preds),
                           torch.log(labels)))
    return rmse.item()
```

$$
\sqrt{\frac{1}{n}\sum_{i=1}^{n}(logy_i - log\hat{y}_i)^2}
$$

与前面的部分不同，我们的训练函数将借助Adam优化器 （我们将在后面章节更详细地描述它）。 Adam优化器的主要吸引力在于它对初始学习率不那么敏感。

```py
def train(net, train_features, train_labels, test_features, test_labels,
          num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    # 这里使用的是Adam优化算法
    optimizer = torch.optim.Adam(net.parameters(),
                                 lr = learning_rate,
                                 weight_decay = weight_decay)
    for epoch in range(num_epochs):
        for X, y in train_iter:
            optimizer.zero_grad()
            l = loss(net(X), y)
            l.backward()
            optimizer.step()
        train_ls.append(log_rmse(net, train_features, train_labels))
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
    return train_ls, test_ls
```

> 解析：
>
> 1. .append()：列表的方法，在列表末尾加元素。
> 2. 函数返回的是每个epoch训练后的损失列表

#### 4.10.6 K折交叉验证

* K折交叉验证：

  1. 每次使用其中 **1 个子集**作为验证集，其余 **K-1 个子集**作为训练集。

  2. 重复 K 次，确保每个子集都被用作一次验证集。

  3. 最终计算 K 次验证结果的平均值，作为模型的性能评估。

```py
def get_k_fold_data(k, i, X, y):
    assert k > 1
    fold_size = X.shape[0] // k
    X_train, y_train = None, None
    for j in range(k):
        idx = slice(j * fold_size, (j + 1) * fold_size)
        X_part, y_part = X[idx, :], y[idx]
        if j == i:
            X_valid, y_valid = X_part, y_part
        elif X_train is None:
            X_train, y_train = X_part, y_part
        else:
            X_train = torch.cat([X_train, X_part], 0)
            y_train = torch.cat([y_train, y_part], 0)
    return X_train, y_train, X_valid, y_valid
```

> 解析：把除了第i折以外的训练数据合并，成为x_train和y_train。第i折数据用于验证。

当我们在K折交叉验证中训练K次后，返回训练和验证误差的平均值。

```py
def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,
           batch_size):
    train_l_sum, valid_l_sum = 0, 0
    for i in range(k):
        data = get_k_fold_data(k, i, X_train, y_train)
        net = get_net()
        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,
                                   weight_decay, batch_size)
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_ls[-1]
        if i == 0:
            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],
                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],
                     legend=['train', 'valid'], yscale='log')
        print(f'折{i + 1}，训练log rmse{float(train_ls[-1]):f}, '
              f'验证log rmse{float(valid_ls[-1]):f}')
    return train_l_sum / k, valid_l_sum / k
```

> 解析：
>
> 1. i从0到k-1，每次都能得到一组train_ls和valid_ls。
> 2. train_l_sum += train_ls[-1]: 这里的[-1]是因为train返回的是损失列表，最后一个元素才是最终损失。

#### 4.10.7 模型选择

```py
k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64
train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,
                          weight_decay, batch_size)
print(f'{k}-折验证: 平均训练log rmse: {float(train_l):f}, '
      f'平均验证log rmse: {float(valid_l):f}')
```

```py
折1，训练log rmse0.170212, 验证log rmse0.156864
折2，训练log rmse0.162003, 验证log rmse0.188812
折3，训练log rmse0.163810, 验证log rmse0.168171
折4，训练log rmse0.167946, 验证log rmse0.154694
折5，训练log rmse0.163320, 验证log rmse0.182928
5-折验证: 平均训练log rmse: 0.165458, 平均验证log rmse: 0.170293
```

以下是折1训练时的损失情况

![折1训练时的损失情况](https://zh-v2.d2l.ai/_images/output_kaggle-house-price_1852a7_170_1.svg)

#### 4.10.8 总结

1. 通过url下载数据，压缩文件
2. 常用的预处理方法：将实值数据重新缩放为零均值和单位方法；用均值替换缺失值。
3. 使用K折交叉验证来选择模型并调整超参数。
4. 对数对于相对误差很有用。

## 5 深度学习计算

### 5.1 层和块

多个层被组合成块，形成更大的模型![](https://zh-v2.d2l.ai/_images/blocks.svg)

```py
import torch
from torch import nn
from torch.nn import functional as F
```

#### 5.1.1 自定义块

在下面的代码片段中，我们从零开始编写一个块。 它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。 注意，下面的`MLP`类继承了表示块的类。 我们的实现只需要提供我们自己的构造函数（Python中的`__init__`函数）和前向传播函数。

```py
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
```

##### 5.1.2 4顺序块

```py
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
```

> 解析：
>
> 1. `_module`的类型是OrderedDict（有序字典），用str(idx)命名是默认名称，nn.Sequential()会这样帮你自动命名，上面是在模拟nn.Sequential()。
> 2. `OrderedDict` 和常规的 Python 字典一样，有一个 `.values()` 方法，返回一个包含字典所有值的视图对象。在这里，`.values()` 会返回所有模块对象（例如 `nn.Linear`、`nn.ReLU`）。相当于剔除掉了键，只保留值。
> 3. nn.Sequential的工作机理：把各种层记录存入_modules有序字典中，在前向传播时，再通过该字典按顺序处理数据。

##### 5.1.3 在前向传播函数中执行代码

```py
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

> 解析：` X = F.relu(torch.mm(X, self.rand_weight) + 1)`  X的大小为(batch_size, 20)

```py
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

这两段代码想要表达的是：块的定义和前向传播时很灵活的。

### 5.2 参数管理

**重要知识点**：
$$
y = xA^T + b
$$
y和x都是行向量，矩阵得右乘。至于为什么用转置，似乎是线代的规定，如果是列向量就是正常矩阵。

#### 5.2.1 参数访问

##### 5.2.1.1目标参数

本节提取的知识点：

1. 每一层都有一个有序字典`state_dict()`，键是参数名称，值是参数的值。并且这些参数都作为**层的属性**存在，可以用`.`访问。
2. 参数是复合的对象，包含值、梯度和额外信息。常见参数有bias和weight，均属于net.parameter.Parameter类。常见指令有`net[0].weight.data` ,`net[2].bias.grad`

##### 5.2.1.2 一次性访问所有参数

比较访问第一个全连接层的参数和访问所有层：

```py
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])
```

```py
('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
```

> 解析：
>
> 1. `.named_parameters()` 是一个是生成器。
> 2. *表解包，可以简单理解为把容器内的元素都取出来。

另一种访问网络参数的方式：

```py
net.state_dict()['2.bias'].data
```

* 对比：

  `named_parameters()` 主要用于访问和操作模型的**可训练参数**，它是一个生成器，可以方便地遍历模型的各个参数。

  `state_dict()` 用于获取**模型的所有参数和状态信息**，它包含了训练的所有参数和缓冲区（比如均值、方差等），通常用于保存和加载模型。

##### 5.2.1.3 从嵌套块收集参数

例如以下嵌套块：

```py
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
```

因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。 下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。

```py
rgnet[0][1][0].bias.data
```

```py
tensor([ 0.1999, -0.4073, -0.1200, -0.2033, -0.1573,  0.3546, -0.2141, -0.2483])
```

#### 5.2.2 参数初始化

知道了如何访问参数后，现在我们看看如何正确地初始化参数。其实我们在之前就已经遇到过类似代码。

##### 5.2.2.1. 内置初始化

下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量， 且将偏置参数设置为0。

```py
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```

```py
(tensor([-0.0214, -0.0015, -0.0100, -0.0058]), tensor(0.))
```

我们还可以将所有参数**初始化为给定的常数**，比如初始化为1。

```py
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
```

```py
(tensor([1., 1., 1., 1.]), tensor(0.))
```

我们还可以**对某些块应用不同的初始化方法**。 例如，下面我们使用Xavier初始化方法初始化第一个神经网络层， 然后将第三个神经网络层初始化为常量值42。

```py
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight) #xavier的均匀分布形式
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(init_xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
```

```py
tensor([ 0.5236,  0.0516, -0.3236,  0.3794])
tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])
```

##### 5.2.2.2 自定义初始化

$$
w \sim \left\{
\begin{matrix}
U(5,10) & \text 可能性\frac{1}{4} \\
0 & \text 可能性\frac{1}{2} \\
U(-10,-5) & \text 可能性\frac{1}{4} \\
\end{matrix}
\right.
$$

```py
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
net[0].weight[:2]
```

```py
Init weight torch.Size([8, 4])
Init weight torch.Size([1, 8])
tensor([[5.4079, 9.3334, 5.0616, 8.3095],
        [0.0000, 7.2788, -0.0000, -0.0000]], grad_fn=<SliceBackward0>)
```

> 解析：
>
> 1. ` m.weight.data *= m.weight.data.abs() >= 5` 权重绝对值大于等于5的不变，小于5的归零。即(-5,5)归零。
> 2. `*[(name, param.shape) for name, param in m.named_parameters()][0]`取出权重。
> 3. 权重矩阵shape为(input_size, output_size)

#### 5.2.3 参数绑定

多个层可以共享参数，参数值相同且修改某一层的值会更新其他共享层的值。

```py
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
```

```py
tensor([True, True, True, True, True, True, True, True])
tensor([True, True, True, True, True, True, True, True])
```

> 解析：共享层的参数指向同一片内存区域，和*引用*其实差不多。

### 5.3 延后初始化

在定义模型时，我们通常无法得知网络的输入维度是什么，这样似乎没有足够的信息来指定输出和模型参数。

* **延后初始化 (defers initialization)** 技术能推迟参数初始化的时机。直到首次传入数据后，才初始化参数。旨在输入维度未知的情况下，预定义灵活的模型，动态推断各个层的参数大小。

#### 5.3.1 手动实现

```py
import torch
import torch.nn as nn


class LazyLinear(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = None  # 不在初始化时设置 Linear 层

    def forward(self, data):
        if self.linear is None:
            self.linear = nn.Linear(in_features=data.shape[-1], out_features=10)  # 根据输入的形状初始化 Linear 层
        return self.linear(data)


if __name__ == '__main__':
    model = LazyLinear()
    print(f'初始化前：\n{model = }')
    print(f'初始化并前向传播：\n{model(torch.randn(5, 20)).shape = }')
    print(f'初始化后：\n{model = }')
```

```py
初始化前：
model = LazyLinear(
  (linear): LazyLinear(in_features=0, out_features=10, bias=True)
)
初始化并前向传播：
model(torch.randn(5, 20)).shape = torch.Size([5, 10])
初始化后：
model = LazyLinear(
  (linear): Linear(in_features=20, out_features=10, bias=True)
)
```

> 解析：
>
> 1. `print(f'初始化前：\n{model = }')`这个语法会自动把model填在”=“后面。
> 2. `torch.randn(5, 20)`这个没看懂，但输入维度是20. randn是random normal的缩写。

#### 5.3.2 自动实现

```py
import torch
import torch.nn as nn


class LazyLinear(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.LazyLinear(out_features=10)  # 使用延后初始化线性层

    def forward(self, x):
        return self.linear(x)


if __name__ == '__main__':
    # import warnings; warnings.filterwarnings("ignore", category=UserWarning)

    model = LazyLinear()
    print(f'初始化前：\n{model = }')
    print(f'初始化并前向传播：\n{model(torch.randn(5, 20)).shape = }')
    print(f'初始化后：\n{model = }')
```

```py
初始化前：
model = LazyLinear(
  (linear): LazyLinear(in_features=0, out_features=10, bias=True)
)
初始化并前向传播：
model(torch.randn(5, 20)).shape = torch.Size([5, 10])
初始化后：
model = LazyLinear(
  (linear): Linear(in_features=20, out_features=10, bias=True)
)
```

总结：主要还是看代码运行结果，有一个直观感受，这一节也没有视频。

### 5.4 自定义层

在5.1节里我们实现了自定义块，自定义层同理，层是比块更低级的抽象。所以本节可以看作我们在自定义神经网络的最基本组件，也就是层（2025.2.4）

#### 5.4.1 不带参数的层

```py
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```

测试：

```py
layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))
```

```py
tensor([-2., -1.,  0.,  1.,  2.])
```

#### 5.4.2 带参数的层

```py
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

> 解析：这里的weight是(input_size, output_size)，和API里weight的shape不同，毕竟前向传播的公式就已经不一样了。

测试：

```py
linear = MyLinear(5, 3)
linear.weight
```

```py
Parameter containing:
tensor([[ 0.1775, -1.4539,  0.3972],
        [-0.1339,  0.5273,  1.3041],
        [-0.3327, -0.2337, -0.6334],
        [ 1.2076, -0.3937,  0.6851],
        [-0.4716,  0.0894, -0.9195]], requires_grad=True)
```

### 5.5 读写文件

我们希望保存训练的模型， 以备将来在各种环境中使用（比如在部署中进行预测）。 此外，当运行一个耗时较长的训练过程时， 最佳的做法是定期保存中间结果， 以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。 因此，现在是时候学习如何加载和存储权重向量和整个模型了。

#### 5.5.1 加载和保存张量

`torch.save()`用于保存，`torch.load()`用于读入。下面是几个例子

```py
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file')
x2 = torch.load('x-file')
x2
```

```py
tensor([0, 1, 2, 3])
```

我们也可以存储一个张量列表：

```py
y = torch.zeros(4)
torch.save([x, y],'x-files')
x2, y2 = torch.load('x-files')
(x2, y2)
```

```py
(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))
```

我们甚至可以写入或读取从字符串映射到张量的字典：

```py
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2
```

```py
{'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])}
```

#### 5.5.2 加载和保存模型参数

保存模型参数而不是保存整个模型，因为模型本身可以包含任意代码，所以模型本身难以序列化。而且我们训练的是参数，保存参数其实更有意义。

```py
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
```

```py
torch.save(net.state_dict(), 'mlp.params')
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
```

```py
MLP(
  (hidden): Linear(in_features=20, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
)
```

> 解析：
>
> 1. `torch.save(net.state_dict(), 'mlp.params')`save的是state_dict()。
> 2. `clone.load_state_dict(torch.load('mlp.params'))` 通过`load_state_dict()`来把参数load到模型上。

### 5.6 GPU

在PyTorch中，CPU和GPU可以用`torch.device('cpu')` 和`torch.device('cuda')`表示。

#### 5.6.1 张量与GPU

默认存储在CPU上：

```py
x = torch.tensor([1, 2, 3])
x.device
```

```py
device(type='cpu')
```

我们也可以存储在GPU上：

```py
X = torch.ones(2, 3, device=torch.device('cuda:0'))
X
```

```py
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
```

我们也可以从一个GPU复制到另一个GPU上：

```py
Z = X.cuda(1)
print(X)
print(Z)
```

```py
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:1')
```

> 解析：`.cuda()` 将模型或张量从 CPU 移动到 GPU，以便利用 GPU 的并行计算能力加速运算。

#### 5.6.2 神经网络与GPU

可将模型参数放在GPU上，这点和[保存模型参数](#5.5.2 加载和保存模型参数)类似。

```py
net = net.Sequential(nn.Linear(3,1))
net = net.to(device=torch.device('cuda'))
```

数据：

```py
net(X)
```

```py
tensor([[-0.4275],
        [-0.4275]], device='cuda:0', grad_fn=<AddmmBackward0>)
```

模型参数：

```py
net[0].weight.data.device
```

```py
device(type='cuda', index=0)
```

可以看到数据和模型参数都在同一个cuda上。

## 6 卷积神经网络

### 6.1 从全连接层到卷积

$$
全连接层 + 平移不变性 + 局部性 = 卷积层
$$

#### 6.1.1 全连接层的四维张量表示

重点在于建立对这个知识点的直观感受：对于下面的全连接层公式（最原始、不加修改的层），每一个输出$H_{i,j}$都有一个对应的矩阵W，令该矩阵和输入$X_{i+a,j+b}$ (a和b表x的移动范围)相乘即可得到该输出。
$$
[H]_{i,j} = [U]_{i,j} + \sum_{k}\sum_{l}[W]_{i,j,k,l}[X]_{k,l} \\
		  = [U]_{i,j} + \sum_{k}\sum_{l}[W]_{i,j,a,b}[X]_{i+a,j+b}
$$
从第一个公式到第二个公式的区别在于，第一个公式默认输入X的所有元素都参与运算(全连接层)，推导到第二个公式其实就是以(i, j)为原坐标，a和b作为偏移量来表示。有点相对坐标和绝对坐标的意思。W可以理解为一个大矩阵，内部元素是小矩阵，(i,j)是大矩阵元素(小矩阵)的坐标，(a,b)是小矩阵(标量)的坐标。

#### 6.1.2 平移不变性

理解了以上的内容，下面的公式也就不难理解，相当于一个矩阵[V]就干了[W]内部所有小矩阵干的事情，前提是所有小矩阵的内容一致。这样系数就少了很多
$$
[H]_{i,j} = u + \sum_{a}\sum_{b}[V]_{a,b}[X]_{i+a,j+b}
$$

#### 6.1.3 局部性

更进一步的，我们不需要偏离到距(i, j)很远的地方，这意味着在|a|>Δ或|b|>Δ的范围之外，我们可以设置$[V]_{a,b}$=0。
$$
[H]_{i,j} = u + \sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}[V]_{a,b}[X]_{i+a,j+b}
$$

#### 6.1.4 通道

输入从二维张量扩展成三维张量：
$$
[H]_{i,j,d} = \sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}\sum_{c}[V]_{a,b,c,d}[X]_{i+a,j+b,c}
$$
c表输入通道数(channel)，是每组卷积核的二维张量数；d表卷积核的组数。

### 6.2 图像卷积

#### 6.2.1 互相关运算和卷积

我们的卷积层其实在做互相关运算而不是卷积计算。为了得到正式的卷积运算输出，我们需要执行中定义的严格卷积运算。幸运的是，它们差别不大，我们只需水平和垂直翻转二维卷积核张量，然后对输入张量执行互相关运算。关于这里**翻转**的理解，可以看3b1b的视频。

为了与深度学习文献中的标准术语保持一致，我们将继续把“互相关运算”称为卷积运算，尽管严格地说，它们略有不同。 此外，对于卷积核张量上的权重，我们称其为*元素*。

#### 6.2.2 卷积层实现

下面是二维互相关运算的代码，也是卷积层实际执行的操作：

```py
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """计算二维互相关运算"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```

> 解析：
>
> 1. 输出shape公式为(n-k+1)。
> 2. `Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))`

下面是卷积层的实现：

```py
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```

> 解析：weight存储卷积核的数据

#### 6.2.3 学习卷积核

```py
# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# 这个二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度），
# 其中批量大小和通道数都为1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # 学习率

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # 迭代卷积核
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')
```

```py
epoch 2, loss 6.422
epoch 4, loss 1.225
epoch 6, loss 0.266
epoch 8, loss 0.070
epoch 10, loss 0.022
```

十次迭代后（这里相当于一个样本训练了十次）

```py
conv2d.weight.data.reshape((1, 2))
```

```py
tensor([[ 1.0010, -0.9739]])
```

#### 6.2.4 感受野

在卷积神经网络中，对于某一层的任意元素x，其*感受野*（receptive field）是指在前向传播期间可能影响x计算的所有元素（来自**所有先前层**）。

感受野最好按从输出到输入的顺序观察，直观感受就是越来越大，像一个锥子。较好数出。

### 6.3 填充和步幅

本节概念很好理解，关键是把写代码时需要计算的公式记住。

$下标h表height，下标w表weight；n表输入维度，k表卷积核维度，p表填充维度$

**重要：代码里的padding是$p_h/2$ (如果整除的话)**

#### 6.3.1 填充

目的：减少边缘像素的损失。（当然使用小卷积核也是一种办法）

* 我们添加$p_h$行填充（大约一半在顶部，一半在底部）和$p_w$列填充（左侧大约一半，右侧一半），则输出形状将为:

$$
(n_h - k_h + p_h + 1) / 1 \ \times \ (n_w - k_w + p_w + 1) / 1 \quad (1)
$$

* 在许多情况下，我们需要设置$p_h=k_h-1,p_w=k_w−1 \quad(2)$，**使输入和输出具有相同的高度和宽度。**卷积神经网络中卷积核的高度和宽度通常为**奇数**，例如1、3、5或7。 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列（p整除2）。

以上两个公式可只记住第二个，第一个公式其实是步幅为1的特殊情况。

```py
import torch
from torch import nn


# 为了方便起见，我们定义了一个计算卷积层的函数。
# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数
def comp_conv2d(conv2d, X):
    # 这里的（1，1）表示批量大小和通道数都是1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # 省略前两个维度：批量大小和通道
    return Y.reshape(Y.shape[2:])

# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape
```

```py
torch.Size([8, 8])
```

> 解析：
>
> 1. `X = X.reshape((1, 1) + X.shape)`：X是二维张量，元组相加是连接而不是真的相加，如：X.shape = (2,2) ,则变换后X.shape = (1,1,2,2)。至于为什么要这么做，其实和前面的多层感知机是一个道理，在多层感知机中，输入第一维表批量大小，剩下表特征；在卷积层中，输入第一维同样表批量大小，剩下三维也用于表特征，有通道数、高度、宽度。
> 2. 注意，这里每边都填充了1行或1列，因此总共添加了2行或2列。
> 3. `nn.Conv2d(1, 1, kernel_size=3, padding=1)` 这里的1,1表输入通道数和输出通道数，其实就是在建立基本的卷积层初始化。
> 4. 8 = 8 - 3 + 2 + 1 /1 

#### 6.3.2 步幅

字面意思，输出形状为：
$$
\lfloor(n_h - k_h + p_h + s_h) / s_h \rfloor \ \times \ \lfloor(n_w - k_w + p_w + s_w) / s_w \rfloor
$$
和公式(1)类似，向下取整可以记忆为“加了一个$s_h$太多得向下取整”

看两个例子：

1. (8 - 3 + 2 + 2 / 2) 向下取整 = 4

```py
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
comp_conv2d(conv2d, X).shape
```

```py
torch.Size([4, 4])
```

2.  (8 - 3 + 0 + 3 / 3)  向下取整 = 2

    (8 - 5 + 2 + 4 /4) 向下取整 = 2

```py
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
```

```py
torch.Size([2, 2])
```

为了简洁起见，当输入高度和宽度两侧的填充数量分别为p_h和p_w时，我们称之为填充(p_h,p_w)。当p_h=p_w=p时，填充是p。同理，当高度和宽度上的步幅分别为s_h和s_w时，我们称之为步幅(s_h,s_w)。特别地，当s_h=s_w=s时，我们称步幅为s。默认情况下，填充为0，步幅为1。在实践中，我们很少使用不一致的步幅或填充，也就是说，我们通常有p_h=p_w和s_h=s_w。

### 6.4 多输入多输出通道

输入X ：$c_i \times n_h \times n_w$

卷积核K：$c_o \times c_i \times k_h \times k_w$

输出Y：$c_o \times m_h \times m_w$

**本节可以理解为nn.conv2d()的手动实现**

#### 6.4.1 多输入通道

```py
import torch
from d2l import torch as d2l

def corr2d_multi_in(X, K):
    # 先遍历“X”和“K”的第0个维度（通道维度），再把它们加在一起
    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))
```

> 解析：
>
> 1. zip()功能：返回一个元组的迭代器(python3)，将若干个可迭代对象的元素组合成若干个新元组，具体看例子。
> 2. x是二维张量，k是对应卷积核，二维。`sum(d2l.corr2d(x, k) for x, k in zip(X, K))`将每个输入通道卷积后的值加起来。也就是把通道融合起来了。
> 3. d2l.corr2d()是[6.2节](#6.2 图像卷积)实现的函数。

#### 6.4.2 多输出通道

多输入的升级版：

```py
def corr2d_multi_in_out(X, K):
    # 迭代“K”的第0个维度，每次都对输入“X”执行互相关运算。
    # 最后将所有结果都叠加在一起
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
```

> 解析：这里的K就是正式版的K，shape为$c_o \times c_i \times k_h \times k_w$，k.shape为$c_i \times k_h \times k_w$ 

### 6.5 池化层

目的：降低卷积层对位置的敏感性

#### 6.5.1 最大池化层和平均池化层

功能：计算窗口中输入子张量的最大值或平均值。

![](https://zh-v2.d2l.ai/_images/pooling.svg)

下面是其手动实现：

```py
import torch
from torch import nn
from d2l import torch as d2l

def pool2d(X, pool_size, mode='max'):
    p_h, p_w = pool_size
    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
    return Y
```

> 解析：`Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))`

#### 6.5.2 填充和步幅

输入张量有四个维度：第一个维度是样本数。

```py
X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
X
```

```py
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
```

默认情况下，深度学习框架中的步幅与汇聚窗口的大小相同。因此，如果我们使用形状为`(3, 3)`的汇聚窗口，那么默认情况下，我们得到的步幅形状为`(3, 3)`。

```py
pool2d = nn.MaxPool2d(3)
pool2d(X)
```

```py
tensor([[[[10.]]]])
```

填充和步幅可以手动设定。

```py
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```

```py
tensor([[[[ 5.,  7.],
          [13., 15.]]]])
```

也可以设定一个任意大小的矩形汇聚窗口，并分别设定填充和步幅的高度和宽度。

```py
pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
pool2d(X)
```

```py
tensor([[[[ 5.,  7.],
          [13., 15.]]]])
```

**总结：上面三种情况从简到繁，主要还是注意默认设定**

#### 6.5.3 多个通道

将X改为具有两个通道的输入（dim=0是样本数）：

```py
X = torch.cat((X, X + 1), 1)
X
```

```py
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])
```

汇聚后输出通道的数量仍是2。

```py
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```

```py
tensor([[[[ 5.,  7.],
          [13., 15.]],

         [[ 6.,  8.],
          [14., 16.]]]])
```

小结：

- 汇聚层的主要优点之一是减轻卷积层对位置的过度敏感。
- 卷积层融合输入通道但池化层不融合。

### 6.6 卷积神经网络(LeNet)

**卷积神经网络主线：逐渐降低空间分辨率，同时增加通道数**

#### 6.6.1 LeNet

架构如图：

![](https://zh-v2.d2l.ai/_images/lenet.svg)

```py
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```

> 解析：一开始输入为1x28x28，经过一个卷积层（28+4-5+1=28）得到6x28x28，经过一个池化层
>
> （28-2+2 / 2 = 14）得到6x14x14，经过第二个卷积层（14-5+1=10）得到16x10x10，经过第二个池化层得到（10-2+2/2 = 5）得到16x5x5，再摊平成全连接层。
>
> 原文对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。

下面通道打印层的名字和输出形状来检查模型：

```py
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

```py
Conv2d output shape:         torch.Size([1, 6, 28, 28])
Sigmoid output shape:        torch.Size([1, 6, 28, 28])
AvgPool2d output shape:      torch.Size([1, 6, 14, 14])
Conv2d output shape:         torch.Size([1, 16, 10, 10])
Sigmoid output shape:        torch.Size([1, 16, 10, 10])
AvgPool2d output shape:      torch.Size([1, 16, 5, 5])
Flatten output shape:        torch.Size([1, 400])
Linear output shape:         torch.Size([1, 120])
Sigmoid output shape:        torch.Size([1, 120])
Linear output shape:         torch.Size([1, 84])
Sigmoid output shape:        torch.Size([1, 84])
Linear output shape:         torch.Size([1, 10])
```

#### 6.6.2 模型训练

```py
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
```

为了进行评估，我们需要对前面的`evaluate_accuracy`函数进行轻微的修改：

```PY
def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """使用GPU计算模型在数据集上的精度"""
    if isinstance(net, nn.Module):
        net.eval()  # 设置为评估模式
        if not device:
            device = next(iter(net.parameters())).device
    # 正确预测的数量，总预测的数量
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERT微调所需的（之后将介绍）
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

> 解析：accuracy()的定义见3.6节。

我们的train函数也需要做出改动：

```py
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """用GPU训练模型(在第六章定义)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # 训练损失之和，训练准确率之和，样本数
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```

> 解析：
>
> 1. 步骤：xavier初始化权重；选择优化器、损失函数、网络设置为训练模式；优化器清空grad，数据移动到GPU，计算损失，反向传播，优化参数。
> 2. 画图其实一点都不会，基本没看画图相关的代码。
