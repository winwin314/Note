# 李沐深度学习笔记

*前言：复习到第三节时才想起来用这个软件，所以笔记从第三节开始*

---

## 3. 线性神经网络

### 3.1 线性回归

> 向量x对应于单个数据样本的特征。 用符号表示的矩阵X可以很方便地引用我们整个数据集的个样本。 其中，X的每一行是一个样本，每一列是一种特征。

*  ==解析解==：可以直接解出损失函数的最小值（求导，导数为0点），从而对应于一个参数w。 这个参数就可以使得损失函数最小，总的来说，就是一步到位，直接找到最优解。
*  ==超参数==：超参数是影响模型性能的重要因素，选择合适的超参数可以帮助模型更好地拟合数据并避免过拟合，
    调节超参数是一项反复试验的工作，通常需要通过不同的技术和方法来进行优化。

* ==学习率==：它控制着每次参数更新的步长,学习率决定了每次更新时沿着梯度方向迈出多大的步伐。

* ==矢量化加速==：利用矢量运算（加减乘除）替代某些运算来实现运行加速的方法。

* ==极大似然估计==：还没学概率论前对这部分根本看不懂，这下看懂了。不过这里是“极大”，通过极大似然估计，

  可以找到参数局部最优值。这些参数计算得来的损失值就是更新后的、减小的损失值。

### 3.2 线性回归的从零实现

* `torch.normal(mean, std, size)` :生成数据服从正态分布，均值mean，方差std，大小为size的矩阵
  eg: torch. normal ( 0, 1, ( num_examples, len( w ) ) )  和  torch. normal( 0, 0.01, y. shape )

* `matmul()`可以实现更普适的张量乘法。 mm只适用于二维矩阵相乘。 

* 特征：X , 权重：***w*** , 标签：y

* 读取数据集（数据迭代器）：

  ```python
  def data_iter(batch_size, features, labels):
      num_examples = len(features)
      indices = list(range(num_examples))
      # 这些样本是随机读取的，没有特定的顺序,shuffle()用于打乱下标
      random.shuffle(indices)
      for i in range(0, num_examples, batch_size):
          batch_indices = torch.tensor(
              indices[i: min(i + batch_size, num_examples)])
          yield features[batch_indices], labels[batch_indices]
  ```

* 定义优化算法（优化器）：

  ```py
  def sgd(params, lr, batch_size):  #@save
      """小批量随机梯度下降"""
      with torch.no_grad():
          for param in params:
              param -= lr * param.grad / batch_size # 在函数外部要先反向传递
              param.grad.zero_()
  ```

* 训练：

  ```py
  lr = 0.03
  num_epochs = 3
  net = linreg
  loss = squared_loss
  
  for epoch in range(num_epochs):
      for X, y in data_iter(batch_size, features, labels):
          l = loss(net(X, w, b), y)  # X和y的小批量损失
          # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
          # 并以此计算关于[w,b]的梯度
          l.sum().backward() # 先反向传递才能得到grad，且这里是多对多所以要用sum()
          sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
      with torch.no_grad():
          train_l = loss(net(features, w, b), labels)
          print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
          #这里的误差会随着每一次更新越来越小
  ```

> 核心是小批量随机梯度下降：选取小批量的损失来计算梯度，以此更新整体的参数

### 3.3 线性回归的简洁实现

**本节核心** :实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。

* 生成数据集：

  ```py
  def load_array(data_arrays, batch_size, is_train=True):  #@save
      """构造一个PyTorch数据迭代器"""
      dataset = data.TensorDataset(*data_arrays) 
      return data.DataLoader(dataset, batch_size, shuffle=is_train)
  	#这个函数里的有关data的类和方法都用于处理data_arrays.
  
  batch_size = 10
  data_iter = load_array((features, labels), batch_size)
  ```

  >  解析：
  >
  > 1. `data.TensorDataset(*data_arrays)` 相当于 `data.TensorDataset(features, labels)` ,传入张量的元组，使用*来解包成两个张量。
  >
  > 2. `data.TensorDataset()` 里的两个张量第一维必须相等。

* 定义模型：

  ```py
  # nn是神经网络的缩写
  from torch import nn
  
  net = nn.Sequential(nn.Linear(2, 1))
  ```

  > 解析：在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。 但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉“标准的流水线”。

* 初始化模型参数：

  ```py
  net[0].weight.data.normal_(0, 0.01)
  net[0].bias.data.fill_(0)
  ```

* 定义损失函数(均方误差)：

  ```py
  loss = nn.MSELoss()
  ```

* 定义优化算法：

  ```py
  trainer = torch.optim.SGD(net.parameters(), lr=0.03)
  ```
  
  > 解析：优化算法只需传入params，见优化算法的具体实现

* 训练：

  ```py
  num_epochs = 3
  for epoch in range(num_epochs):
      for X, y in data_iter:
          l = loss(net(X) ,y)	# X,y是部分样本
          trainer.zero_grad()
          l.backward()
          trainer.step()
      l = loss(net(features), labels) #用部分样本优化的模型来计算关于全部样本的损失
      print(f'epoch {epoch + 1}, loss {l:f}')
  ```

### 3.4 softmax回归

* softmax函数：能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。
  $$
  softmax(o_j) = \frac{exp(o_j)}{\sum_{k}^{} exp(o_k)}
  $$

* 小批量样本的矢量化：
  $$
  O = XW + b,\\
  \hat{Y} = softmax(O)
  $$





